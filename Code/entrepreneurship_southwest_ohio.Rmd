---
title: "Understanding and Quantifying Impactful Research Published in JQT"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author:
  - name: "Greg Niemesh ^[Email: niemesgt@miamioh.edu | Phone: +1-513-529-42150 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/niemesgt\">Miami University Official</a> ]"
    affiliation: Farmer School of Business, Miami University
  - name: "Fadel M. Megahed ^[Email: fmegahed@miamioh.edu | Phone: +1-513-529-4185 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/megahefm\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
bibliography: refs.bib
link-citations: yes
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      out.width = "100%",
                      warning = FALSE,
                      message = FALSE,
                      results = "asis") 
```

---

This Markdown document is intended to serve <span style="text-decoration:underline">three</span> main purposes:

* Provides the data and code that we used in our analysis. We believe that providing the code is an important step in ensuring that our analysis can be easily reproduced, which:
  - is an important pillar of the scientific method since it facilitates the evaluation/validation of our analysis.
  - encourages future work in this area.
* Provides the results that we have obtained for each stage of the analysis.
* Allows for reusing our code and reproducing our results, which in our view is a major limitation in the majority of the published literature since they do not provide code and it is often unclear what are some of the decision made in terms of data preprocessing and modeling.

To navigate this Markdown, please feel free to use the navigation bar at the left. The reader can **hide** any code chunk by clicking on the code button. We use hyperlinks to document: (a) data sources used in the analysis, (b) repository for where we saved the results, and (c) relevant **R** and/or **Python** programming resources.


# Data Extraction

The snippet below documents the list of **R** packages and functions that were used in this research. For convenience, we used the `pacman` package since it allows for installing/loading the needed packages in one step. Please make sure that the package is installed on your system using the command `install.packages("pacman")` before running this code chunk.

```{r packages, results='hide'}
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman)
p_load(devtools)
devtools::install_github("hrbrmstr/domaintools")
devtools::install_github("hrbrmstr/wayback")
p_load(httr, rvest, 
       RCurl, jsonlite, reticulate,
       readxl, tidyverse, RColorBrewer,
       tools, DT, formattable,
       gsubfn, anytime, magrittr, 
       domaintools, wayback)

```


## Zip Codes in Southwest Ohio {.tabset .tabset-fade}
To obtain the list of Zip Codes in Southwest Ohio, we have done the following:

1. We downloaded all 1415 Zip Codes in Ohio from [zip-codes.com](https://www.zip-codes.com/state/oh.asp) and stored the result as a csv (where we modified the first column data asfollows: ZIP Code XXXXX -> XXXX);
2. We have identified the 16 counties capturing Southwest Ohio from the [Southwest Ohio Wikipedia Page](https://wikitravel.org/en/Southwest_Ohio). The county names were stored in our R program using the vector named ```county_names```;
3. Using R, we have subseted the CSV file using the following two rules:
- Restricting zip codes to those having a County Identifier in ```county_names```.
- Restricting zip codes to non P.O. Boxes

From our code, we have obtained 220 unique zip codes capturing the 16 counties of Southwest Ohio. These were stored in a CSV titled: "SouthwestOhioZips.csv".


```{r zipCodes}
# County names in Southwest Ohio Per Wiki Travel
county_names <- c("Adams", "Brown","Butler", "Clermont", "Clinton",
                  "Fayette", "Greene", "Hamilton", "Highland", "Montgomery",
                  "Pickaway", "Pike", "Preble", "Ross", "Scioto", "Warren")

# We have downloaded all zip codes in the sate from
# https://www.zip-codes.com/state/oh.asp, which was saved as a csv
oh_ZipCodes <- read.csv("../Data/ohioZipCodes.csv", stringsAsFactors = F) %>% 
  subset(subset = County %in% county_names) %>% subset(Type != "P.O. Box")
ZIPs <- as.character(oh_ZipCodes[,1])
write.csv(ZIPs, "../Data/SouthwestOhioZips.csv", row.names = FALSE)
```

## Identifying the Domains Associated with these Zip Codes

### Scraping Domain Information
In the code chunk below, we will connect to the [Security Trails API](https://securitytrails.com/) to obtain information on the top 500 websites per each Zip Code. Since it was not obvious how to identify the number of domains per website using the API, we have selected 500 since it:

* Is a large enough number, i.e. 500 business per Zip Code will exceed the number of businesses located in most of these zip codes
* Allows us to be under the 1500 queries limit, which would require a more expensive API subscription, which for the purpose of this initial analysis seemed to be not necessary.

The requests were made using *Python Programming Language*. The files were saved using a JSON file structure and stored in a text file titled: *ohioWebsites.JSON*.

```{python securitytrails, eval=F}
import requests
import pickle

def save_object(obj, filename):
    with open(filename, 'a+', encoding='utf-8') as file:
        file.write(obj)

# https://docs.securitytrails.com/reference
url = "https://api.securitytrails.com/v1/domains/list"

# Zip Codes: Pasted from the CSV using text editor for our convenience 
postcodes = ["43101", "43103", "43106", "43113", "43115", "43116", "43117", "43128", "43142", "43145", "43146", "43156", "43160", "43164", "45001", "45002", "45003", "45004", "45005", "45011", "45012", "45013", "45014", "45015", "45018", "45030", "45032", "45033", "45034", "45036", "45039", "45040", "45041", "45042", "45044", "45050", "45051", "45052", "45053", "45054", "45055", "45056", "45061", "45062", "45063", "45064", "45065", "45066", "45067", "45068", "45069", "45070", "45071", "45101", "45102", "45103", "45105", "45106", "45107", "45111", "45112", "45113", "45114", "45115", "45118", "45119", "45120", "45121", "45122", "45123", "45130", "45131", "45132", "45133", "45135", "45140", "45142", "45144", "45146", "45147", "45148", "45150", "45152", "45153", "45154", "45155", "45156", "45157", "45158", "45159", "45160", "45162", "45164", "45166", "45167", "45168", "45169", "45171", "45172", "45174", "45176", "45177", "45201", "45202", "45203", "45204", "45205", "45206", "45207", "45208", "45209", "45211", "45212", "45213", "45214", "45215", "45216", "45217", "45218", "45219", "45220", "45221", "45222", "45223", "45224", "45225", "45226", "45227", "45229", "45230", "45231", "45232", "45233", "45234", "45235", "45236", "45237", "45238", "45239", "45240", "45241", "45242", "45243", "45244", "45245", "45246", "45247", "45248", "45249", "45250", "45251", "45252", "45253", "45254", "45255", "45258", "45262", "45263", "45264", "45267", "45268", "45269", "45270", "45271", "45273", "45274", "45275", "45277", "45280", "45296", "45298", "45299", "45301", "45305", "45307", "45309", "45311", "45314", "45315", "45316", "45320", "45321", "45322", "45324", "45325", "45327", "45330", "45335", "45338", "45342", "45343", "45345", "45347", "45354", "45370", "45377", "45378", "45381", "45382", "45384", "45385", "45387", "45401", "45402", "45403", "45404", "45405", "45406", "45409", "45410", "45412", "45413", "45414", "45415", "45416", "45417", "45419", "45420", "45422", "45423", "45424", "45426", "45428", "45429", "45430", "45431", "45432", "45433", "45434", "45435", "45437", "45439", "45440", "45441", "45448", "45449", "45458", "45459", "45469", "45470", "45475", "45479", "45481", "45482", "45490", "45601", "45612", "45613", "45616", "45617", "45618", "45624", "45628", "45629", "45630", "45633", "45636", "45642", "45644", "45646", "45647", "45648", "45650", "45652", "45653", "45657", "45660", "45661", "45662", "45663", "45671", "45673", "45677", "45679", "45681", "45682", "45683", "45684", "45687", "45690", "45693", "45694", "45697", "45699", "45999"]

for postcode in postcodes:
    for x in range(1,6):
        querystring = {"apikey":"7GpQJYMryiL24bulfUWmqiQgeRrExUKx", "page":str(x)}
        payload = "{\"filter\":{\"whois_postalCode\":\"" + postcode +"\"}}"
        response = requests.request("POST", url, data=payload, params=querystring)
        filename = "../Data/ohioWebsites.JSON"
        save_object(response.text, filename)

        # filename = "../Data/ohioWebsites"+postcode+"P"+str(x)+".pickle"
        # save_object(response.text, filename)
```


### Extracting Information from Scraped Data
Based on the analysis above, we have chosen to store all the data in one file.  While this is convenient, it creates the problem of having different (JSON) structures stored within our JSON file, which means it cannot be read using existing packages in either R or JSON. For the purpose of our analysis, this is not important since we only care about the following information within the file:

* **Hostname:** This represents the domain for the website.
* **Created Date:** When was the website first created.
* **Expires Date:** When will the website expire.
* **Alexa Ranking:** The popularity of the website as determined with the Alexa ranking system (which accounts for traffic/ no. of visitors).

The chunk of code below provides the following functionality:

1. Extracting the aforementioned four features for all the southwestern ohio websites.
2. "Prettifying" the data by storing it in a typical CSV format for future analysis.

Note that by just having the domain name, we can extract all the whois information for free using the R package [domaintools](https://github.com/hrbrmstr/domaintools). Thus, if any of the information that we did not extract is needed, we should be able to obtain it using the aforementioned package.

```{r ETL}
messy_json_data <- readLines("../Data/ohioWebsites.JSON")

# Using regular expressions to extract hostname (domain name), created date, expiration date and alexa website popularity data from the messy JSON file
hostname <- strapplyc(messy_json_data, '\"hostname\": "(.+)"', 
                                     simplify = rbind) %>% as.data.frame() 
createdDate <- strapplyc(messy_json_data, '\"createdDate\": (.*)',
                                       simplify = rbind) %>%  as.data.frame()
expiresDate <- strapplyc(messy_json_data, '\"expiresDate\": (.*?),',
                                       simplify = rbind) %>% as.data.frame()
alexarank <- strapplyc(messy_json_data, '\"alexa_rank\": (.*)',
                                       simplify = rbind) %>% as.data.frame()

# Converting Nulls to NAs to allow for converting the matrices to numeric
createdDate[createdDate=="null"] = NA
expiresDate[expiresDate=="null"] = NA
alexarank[alexarank =="null"] = NA
hostname[hostname=="null"] = NA

# Converting from Unix Milliseconds to Understandable Dates
as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
createdDate <- unlist(createdDate) %>% as.numeric.factor() %>% multiply_by(0.001) %>% 
  anydate() %>% as.data.frame()
expiresDate <- unlist(expiresDate) %>% as.numeric.factor() %>% multiply_by(0.001) %>% 
  anydate() %>% as.data.frame()

# Making a table from all four variables and storing the results as a CSV for future processing
websiteTable <- cbind(hostname, createdDate, expiresDate, alexarank) %>%  as.data.frame()
colnames(websiteTable) <- c("hostName", "createdDate", "expiresDate", "alexarank")
write.csv(websiteTable, file = "../Data/CleanedOhioWebsites.csv", row.names = F)
```


# Domain Histories
Based on Section (1), we know have the domain names for approximately 83450 websites that are registered under one of the zip codes corresponding to the counties we explored. In this section, we will explore how each of these domains have changed over time using information from 
