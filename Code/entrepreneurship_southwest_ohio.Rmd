---
title: "Understanding and Quantifying Impactful Research Published in JQT"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author:
  - name: "Greg Niemesh ^[Email: niemesgt@miamioh.edu | Phone: +1-513-529-42150 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/niemesgt\">Miami University Official</a> ]"
    affiliation: Farmer School of Business, Miami University
  - name: "Fadel M. Megahed ^[Email: fmegahed@miamioh.edu | Phone: +1-513-529-4185 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/megahefm\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
bibliography: refs.bib
link-citations: yes
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      out.width = "100%",
                      warning = FALSE,
                      message = FALSE,
                      results = "asis") 
```

---

This Markdown document is intended to serve <span style="text-decoration:underline">three</span> main purposes:

* Provides the data and code that we used in our analysis. We believe that providing the code is an important step in ensuring that our analysis can be easily reproduced, which:
  - is an important pillar of the scientific method since it facilitates the evaluation/validation of our analysis.
  - encourages future work in this area.
* Provides the results that we have obtained for each stage of the analysis.
* Allows for reusing our code and reproducing our results, which in our view is a major limitation in the majority of the published literature since they do not provide code and it is often unclear what are some of the decision made in terms of data preprocessing and modeling.

To navigate this Markdown, please feel free to use the navigation bar at the left. The reader can **hide** any code chunk by clicking on the hide button. We use hyperlinks to document: (a) data sources used in the analysis, (b) repository for where we saved the results, and (c) relevant **R** and/or **Python** programming resources.


# Data Extraction

The snippet below documents the list of **R** packages and functions that were used in this research. For convenience, we used the `pacman` package since it allows for installing/loading the needed packages in one step. Please make sure that the package is installed on your system using the command `install.packages("pacman")` before running this code chunk.

```{r packages, results='hide'}
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman)
p_load(devtools)
devtools::install_github("hrbrmstr/domaintools")
devtools::install_github("hrbrmstr/wayback")
p_load(httr, rvest, 
       RCurl, jsonlite, reticulate,
       readxl, tidyverse, RColorBrewer,
       tools, DT, formattable,
       gsubfn, anytime, magrittr, 
       domaintools, wayback)

```


## Zip Codes in Southwest Ohio {.tabset .tabset-fade}
To obtain the list of Zip Codes in Southwest Ohio, we have done the following:

1. We downloaded all 1415 Zip Codes in Ohio from [zip-codes.com](https://www.zip-codes.com/state/oh.asp) and stored the result in [ohioZipCodes.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/ohioZipCodes.csv) (where we modified the first column data as follows: ZIP Code XXXXX -> XXXX);
2. We have identified the 16 counties capturing Southwest Ohio from the [Southwest Ohio Wikipedia Page](https://wikitravel.org/en/Southwest_Ohio). The county names were stored in our R program using the vector named ```county_names```;
3. Using R, we have subseted the CSV file using the following two rules:
- Restricting zip codes to those having a County Identifier in ```county_names```.
- Restricting zip codes to non P.O. Boxes

From our code, we have obtained 220 unique zip codes capturing the 16 counties of Southwest Ohio. These were stored in [SouthwestOhioZips.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/tree/master/Data).


```{r zipCodes}
# County names in Southwest Ohio Per Wiki Travel
county_names <- c("Adams", "Brown","Butler", "Clermont", "Clinton",
                  "Fayette", "Greene", "Hamilton", "Highland", "Montgomery",
                  "Pickaway", "Pike", "Preble", "Ross", "Scioto", "Warren")

# We have downloaded all zip codes in the sate from
# https://www.zip-codes.com/state/oh.asp, which was saved as a csv
oh_ZipCodes <- read.csv("../Data/ohioZipCodes.csv", stringsAsFactors = F) %>% 
  subset(subset = County %in% county_names) %>% subset(Type != "P.O. Box")
ZIPs <- as.character(oh_ZipCodes[,1])
write.csv(ZIPs, "../Data/SouthwestOhioZips.csv", row.names = FALSE)
```

## Scraping Domain Information Associated with these Zip Codes
In the code chunk below, we will connect to the [Security Trails API](https://securitytrails.com/) to create the following two comma seperated value (CSV) files:

  (A) [meta.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/meta.csv), which stores the following meta data related to our API queries:
  
  * **scrape_date:** Date when we scrapped the WhoIs database using the Security Trails API;
  * **post_code:** The postal code for which websites were being scraped;
  * **record_count:** Number of websites/domain names that are registered for that postal code; 
  * **max_page:** Number of page results on Security Trails that we have to iterate through for this post code. Not that if there are 6 pages, the first five pages will each contain 100 records and the sixth will contain up to 100 records.
  (B) [records.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/records.csv), which stores information in the following variables:
  
  * **scrape_date:** Date when we scrapped the WhoIs database using the Security Trails API;
  * **post_code:** The postal code for which websites were being scraped;
  * **hostName:** domain name for the website; 
  * **createdDate:** when was the website created; 
  * **expiresDate:** when does the current regiseration for the website expire; 
  * **companyName:** this is computed from the WhoIS information, and thus, it is not always exact and contains a lot of *None*.

The requests were made using *Python Programming Language*, as shown in the code snippet below. Note that we were able to use country as a part of our filter search since we had a **Professional Subscription account**.

```{python securitytrails, eval=F}
# Loading required libraries
import requests
import json
from datetime import datetime


# Function to overcome errors in printing Nones
def xstr(s):
    return '' if s is None else str(s)


# Defining how the files will be saved into csv
meta_data_file = open('../Data/meta_data.csv','w')
meta_data_file.write("scrape_date ,post_code ,record_count ,max_page\n")   
records_file = open('../Data/records.csv','w')
records_file.write("scrape_date ,post_code , hostName ,createdDate ,expiresDate ,companyName \n" )


# URL and post codes
url = "https://api.securitytrails.com/v1/domains/list"
# Zip Codes: Pasted from the CSV using text editor for our convenience 
postcodes = ["43101", "43103", "43106", "43113", "43115", "43116", "43117", "43128", "43142", "43145", "43146", "43156", "43160", "43164", "45001", "45002", "45003", "45004", "45005", "45011", "45012", "45013", "45014", "45015", "45018", "45030", "45032", "45033", "45034", "45036", "45039", "45040", "45041", "45042", "45044", "45050", "45051", "45052", "45053", "45054", "45055", "45056", "45061", "45062", "45063", "45064", "45065", "45066", "45067", "45068", "45069", "45070", "45071", "45101", "45102", "45103", "45105", "45106", "45107", "45111", "45112", "45113", "45114", "45115", "45118", "45119", "45120", "45121", "45122", "45123", "45130", "45131", "45132", "45133", "45135", "45140", "45142", "45144", "45146", "45147", "45148", "45150", "45152", "45153", "45154", "45155", "45156", "45157", "45158", "45159", "45160", "45162", "45164", "45166", "45167", "45168", "45169", "45171", "45172", "45174", "45176", "45177", "45201", "45202", "45203", "45204", "45205", "45206", "45207", "45208", "45209", "45211", "45212", "45213", "45214", "45215", "45216", "45217", "45218", "45219", "45220", "45221", "45222", "45223", "45224", "45225", "45226", "45227", "45229", "45230", "45231", "45232", "45233", "45234", "45235", "45236", "45237", "45238", "45239", "45240", "45241", "45242", "45243", "45244", "45245", "45246", "45247", "45248", "45249", "45250", "45251", "45252", "45253", "45254", "45255", "45258", "45262", "45263", "45264", "45267", "45268", "45269", "45270", "45271", "45273", "45274", "45275", "45277", "45280", "45296", "45298", "45299", "45301", "45305", "45307", "45309", "45311", "45314", "45315", "45316", "45320", "45321", "45322", "45324", "45325", "45327", "45330", "45335", "45338", "45342", "45343", "45345", "45347", "45354", "45370", "45377", "45378", "45381", "45382", "45384", "45385", "45387", "45401", "45402", "45403", "45404", "45405", "45406", "45409", "45410", "45412", "45413", "45414", "45415", "45416", "45417", "45419", "45420", "45422", "45423", "45424", "45426", "45428", "45429", "45430", "45431", "45432", "45433", "45434", "45435", "45437", "45439", "45440", "45441", "45448", "45449", "45458", "45459", "45469", "45470", "45475", "45479", "45481", "45482", "45490", "45601", "45612", "45613", "45616", "45617", "45618", "45624", "45628", "45629", "45630", "45633", "45636", "45642", "45644", "45646", "45647", "45648", "45650", "45652", "45653", "45657", "45660", "45661", "45662", "45663", "45671", "45673", "45677", "45679", "45681", "45682", "45683", "45684", "45687", "45690", "45693", "45694", "45697", "45699", "45999"]
# API key for Security Trails
querystring = {"apikey":"7GpQJYMryiL24bulfUWmqiQgeRrExUKx"}


# Querying the API
for postcode in postcodes:    
    payload = "{\"query\":\"whois_country='united states' and  whois_postalCode='" + postcode + "'\"}"
    response = requests.request("POST", url, data=payload, params=querystring)
    json_data = json.loads(response.text)
    
#   Obtaining meta data to identify number of pages and records per postal code
    if 'meta' in json_data:
        max_page = json_data["meta"]["max_page"]
        record_count = json_data['record_count']
        meta_data_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) +"," + xstr(record_count) + "," + xstr(max_page) +"\n") 
    
#   Inner loop --> Going through first page
    if 'records' in json_data:
        for i in range(0, len(json_data['records'])):
            companyName = str(json_data['records'][i]['computed']['company_name']).replace(",", "").replace("/t", "")
            hostName = json_data['records'][i]['hostname']
            expiresDate = json_data['records'][i]['whois']['expiresDate']
            if expiresDate is not None:
                expiresDate = expiresDate * 0.001
                expiresDate = xstr(datetime.utcfromtimestamp(expiresDate).strftime('%Y-%m-%d'))
            else:
                expiresDate = "NA"
                
            createdDate = json_data['records'][i]['whois']['createdDate']
            if createdDate is not None:
                createdDate = createdDate * 0.001
                createdDate = xstr(datetime.utcfromtimestamp(createdDate).strftime('%Y-%m-%d'))
            else:
                createdDate = "NA"
            records_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) + "," + xstr(hostName) + "," + createdDate + "," + expiresDate +"," + xstr(companyName) + "\n" )
#    If pages > 1 --> iterate through remaining pages and records
        if max_page > 1:    
            for pageno in range(2, max_page+1):
                querystring = {"apikey":"7GpQJYMryiL24bulfUWmqiQgeRrExUKx", "page":str(pageno)}
                payload = "{\"query\":\"whois_country='united states' and  whois_postalCode='" + postcode + "'\"}"
                response = requests.request("POST", url, data=payload, params=querystring)
                
                for i in range(0, len(json_data['records'])):
                    companyName = json_data['records'][i]['computed']['company_name']
                    hostName = json_data['records'][i]['hostname']
                    expiresDate = json_data['records'][i]['whois']['expiresDate']
                    if expiresDate is not None:
                        expiresDate = expiresDate * 0.001
                        expiresDate = xstr(datetime.utcfromtimestamp(expiresDate).strftime('%Y-%m-%d'))
                    else:
                        expiresDate = "NA"
                        
                    createdDate = json_data['records'][i]['whois']['createdDate']
                    if createdDate is not None:
                        createdDate = createdDate * 0.001
                        createdDate = xstr(datetime.utcfromtimestamp(createdDate).strftime('%Y-%m-%d'))
                    else:
                        createdDate = "NA"
                    records_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) + "," + xstr(hostName) + "," + createdDate + "," + expiresDate +"," + xstr(companyName) + "\n" )
                
meta_data_file.close()
records_file.close()
```




# Domain Histories
Based on Section (1), we know have the domain names for approximately 131,723 websites that are registered under one of the zip codes corresponding to the counties we explored. In this section, we will explore how each of these domains have changed over time using information from the [Wayback Machine API](https://web.archive.org/). Specifically, we can obtain the number of unique archives for a given webpage using the [waybackpack](https://github.com/jsvine/waybackpack) python-based package for the Wayback Machine API.

## Number of Changes and Archived URLs
As a first step to understanding the domain histories, we capitalized on the [waybackpack](https://github.com/jsvine/waybackpack) to obtain the number of changes for given hostname and the archieved URLs corresponding to unique pages. *For now, we are hypothesizing that the number of changes correspond to growth/economic activity.* **Obviously, we can parse the changes to obtain changes per year or month (we will do this in a future time).** The results from this analysis step are stored in: []()
```{python num_changes, eval=F}
# In this next block of code, we do the following:
# (A) Open the records.csv file from the previous chunk
# (B) Remove links for companies that has more than one link to reduce computational time (with exception of None, NA since it is used for multiple companies)
# (C) Save the resulting data cleaning in: records_unique.csv
with open('../Data/records.csv','r') as in_file, open('../Data/records_unique.csv','w') as out_file:
    seen = set() # set for fast O(1) amortized lookup
    
    for line in in_file:
        cur_company = line.split(",")[5].strip()
        if cur_company in seen: continue # skip duplicate
        if cur_company.lower() != "none" and cur_company is not "NA" and len(cur_company) > 2:
            seen.add(cur_company)
        
        out_file.write(line)

out_file.close()
in_file.close()

# In the next code block, we will capitalize on the subprocess package to run comman line processes so we can capitalize on some of the functionality of the wayback python interface (from the aforementioned GitHub link)


from subprocess import check_output

with open('../Data/records_unique.csv','r') as in_file, open('../Data/wayback_changes.csv','w') as out_file:
    out_file.write("URL, created date, expires date, num changes, wayback URLs\n")
    for line in in_file:
         # Obtaining hostname and dates from records_unique.csv
         url = line.split(",")[2].strip()
         created_date = line.split(",")[3].strip()
         expires_date = line.split(",")[4].strip()
         
         # Ignoring the header
         if  line.split(",")[2].strip() == "hostName":
             continue
         # Running command line using the check_output
         wayback_unique_list = check_output("waybackpack " + url + " --list --uniques-only", shell=True)
         wayback_list = wayback_unique_list.decode('ASCII').strip()
         
         out_file.write(url + "," + created_date + "," + expires_date + ",")
         
         # If we get a URL for the archieve, then we store it and #changes
         if len(wayback_list) != 0:
            wayback_list_as_list = wayback_list.split("\n")
            no_changes = str(len(wayback_list_as_list))
            out_file.write(no_changes + ",")
            # For loop to seperare multiple URLS with semi column
            for wayback_data in wayback_list_as_list:
                 out_file.write(wayback_data.strip() + ";")
            out_file.write("\n")
         else: # #changes equal zero
            out_file.write("0\n")
            
out_file.close()
in_file.close()
```

## Momentos (TBC)
The next step we will explore getting the content of the pages using the ```get_memento()``` function in **R** or the [waybackpack](https://github.com/jsvine/waybackpack) in Python.

