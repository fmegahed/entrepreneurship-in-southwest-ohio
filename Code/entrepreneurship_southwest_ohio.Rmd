---
title: "Examining Entrepreneurship in Southwestern Ohio: An Internet Based Perspective"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author:
  - name: "Greg Niemesh ^[Email: niemesgt@miamioh.edu | Phone: +1-513-529-42150 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/niemesgt\">Miami University Official</a> ]"
    affiliation: Farmer School of Business, Miami University
  - name: "Fadel M. Megahed ^[Email: fmegahed@miamioh.edu | Phone: +1-513-529-4185 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/megahefm\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
bibliography: refs.bib
link-citations: yes
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      out.width = "100%",
                      warning = FALSE,
                      message = FALSE,
                      results = "asis") 
```

---

This Markdown document is intended to serve <span style="text-decoration:underline">three</span> main purposes:

* Provides the data and code that we used in our analysis. We believe that providing the code is an important step in ensuring that our analysis can be easily reproduced, which:
  - is an important pillar of the scientific method since it facilitates the evaluation/validation of our analysis.
  - encourages future work in this area.
* Provides the results that we have obtained for each stage of the analysis.
* Allows for reusing our code and reproducing our results, which in our view is a major limitation in the majority of the published literature since they do not provide code and it is often unclear what are some of the decision made in terms of data preprocessing and modeling.

To navigate this Markdown, please feel free to use the navigation bar at the left. The reader can **hide** any code chunk by clicking on the hide button. We use hyperlinks to document: (a) data sources used in the analysis, (b) repository for where we saved the results, and (c) relevant **R** and/or **Python** programming resources.


# Data Extraction

The snippet below documents the list of **R** packages and functions that were used in this research. For convenience, we used the `pacman` package since it allows for installing/loading the needed packages in one step. Please make sure that the package is installed on your system using the command `install.packages("pacman")` before running this code chunk.

```{r packages, results='hide'}
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman)
p_load(devtools)
devtools::install_github("hrbrmstr/domaintools")
devtools::install_github("hrbrmstr/wayback")
p_load(httr, rvest, 
       RCurl, jsonlite, reticulate,
       readxl, tidyverse, RColorBrewer,
       tools, DT, formattable,
       gsubfn, anytime, magrittr, 
       domaintools, wayback,
       lubridate, dplyr, geojsonio)

```


## Zip Codes in Southwest Ohio {.tabset .tabset-fade}
To obtain the list of Zip Codes in Southwest Ohio, we have done the following:

1. We downloaded all 1415 Zip Codes in Ohio from [zip-codes.com](https://www.zip-codes.com/state/oh.asp) and stored the result in [ohioZipCodes.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/ohioZipCodes.csv) (where we modified the first column data as follows: ZIP Code XXXXX -> XXXX);
2. We have identified the 16 counties capturing Southwest Ohio from the [Southwest Ohio Wikipedia Page](https://wikitravel.org/en/Southwest_Ohio). The county names were stored in our R program using the vector named ```county_names```;
3. Using R, we have subseted the CSV file using the following two rules:
- Restricting zip codes to those having a County Identifier in ```county_names```.
- Restricting zip codes to non P.O. Boxes

From our code, we have obtained 220 unique zip codes capturing the 16 counties of Southwest Ohio. These were stored in [SouthwestOhioZips.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/tree/master/Data).


```{r zipCodes}
# County names in Southwest Ohio Per Wiki Travel
county_names <- c("Adams", "Brown","Butler", "Clermont", "Clinton",
                  "Fayette", "Greene", "Hamilton", "Highland", "Montgomery",
                  "Pickaway", "Pike", "Preble", "Ross", "Scioto", "Warren")

# We have downloaded all zip codes in the sate from
# https://www.zip-codes.com/state/oh.asp, which was saved as a csv
oh_ZipCodes <- read.csv("../Data/ohioZipCodes.csv", stringsAsFactors = F) %>% 
  subset(subset = County %in% county_names) %>% subset(Type != "P.O. Box")
ZIPs <- as.character(oh_ZipCodes[,1])
write.csv(ZIPs, "../Data/SouthwestOhioZips.csv", row.names = FALSE)
```

## Scraping Domain Information Associated with these Zip Codes
In the code chunk below, we will connect to the [Security Trails API](https://securitytrails.com/) to create the following two comma seperated value (CSV) files:

  (A) [meta.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/meta.csv), which stores the following meta data related to our API queries:
  
  * **scrape_date:** Date when we scrapped the WhoIs database using the Security Trails API;
  * **post_code:** The postal code for which websites were being scraped;
  * **record_count:** Number of websites/domain names that are registered for that postal code; 
  * **max_page:** Number of page results on Security Trails that we have to iterate through for this post code. Not that if there are 6 pages, the first five pages will each contain 100 records and the sixth will contain up to 100 records.
  (B) [records.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/records.csv), which stores information in the following variables:
  
  * **scrape_date:** Date when we scrapped the WhoIs database using the Security Trails API;
  * **post_code:** The postal code for which websites were being scraped;
  * **hostName:** domain name for the website; 
  * **createdDate:** when was the website created; 
  * **expiresDate:** when does the current regiseration for the website expire; 
  * **companyName:** this is computed from the WhoIS information, and thus, it is not always exact and contains a lot of *None*.

The requests were made using *Python Programming Language*, as shown in the code snippet below. Note that we were able to use country as a part of our filter search since we had a **Professional Subscription account**.

```{python securitytrails, eval=F}
# Loading required libraries
import requests
import json
from datetime import datetime


# Function to overcome errors in printing Nones
def xstr(s):
    return '' if s is None else str(s)


# Defining how the files will be saved into csv
meta_data_file = open('../Data/meta_data.csv','w')
meta_data_file.write("scrape_date ,post_code ,record_count ,max_page\n")   
records_file = open('../Data/records.csv','w')
records_file.write("scrape_date ,post_code , hostName ,createdDate ,expiresDate ,companyName \n" )


# URL and post codes
url = "https://api.securitytrails.com/v1/domains/list"
# Zip Codes: Pasted from the CSV using text editor for our convenience 
postcodes = ["43101", "43103", "43106", "43113", "43115", "43116", "43117", "43128", "43142", "43145", "43146", "43156", "43160", "43164", "45001", "45002", "45003", "45004", "45005", "45011", "45012", "45013", "45014", "45015", "45018", "45030", "45032", "45033", "45034", "45036", "45039", "45040", "45041", "45042", "45044", "45050", "45051", "45052", "45053", "45054", "45055", "45056", "45061", "45062", "45063", "45064", "45065", "45066", "45067", "45068", "45069", "45070", "45071", "45101", "45102", "45103", "45105", "45106", "45107", "45111", "45112", "45113", "45114", "45115", "45118", "45119", "45120", "45121", "45122", "45123", "45130", "45131", "45132", "45133", "45135", "45140", "45142", "45144", "45146", "45147", "45148", "45150", "45152", "45153", "45154", "45155", "45156", "45157", "45158", "45159", "45160", "45162", "45164", "45166", "45167", "45168", "45169", "45171", "45172", "45174", "45176", "45177", "45201", "45202", "45203", "45204", "45205", "45206", "45207", "45208", "45209", "45211", "45212", "45213", "45214", "45215", "45216", "45217", "45218", "45219", "45220", "45221", "45222", "45223", "45224", "45225", "45226", "45227", "45229", "45230", "45231", "45232", "45233", "45234", "45235", "45236", "45237", "45238", "45239", "45240", "45241", "45242", "45243", "45244", "45245", "45246", "45247", "45248", "45249", "45250", "45251", "45252", "45253", "45254", "45255", "45258", "45262", "45263", "45264", "45267", "45268", "45269", "45270", "45271", "45273", "45274", "45275", "45277", "45280", "45296", "45298", "45299", "45301", "45305", "45307", "45309", "45311", "45314", "45315", "45316", "45320", "45321", "45322", "45324", "45325", "45327", "45330", "45335", "45338", "45342", "45343", "45345", "45347", "45354", "45370", "45377", "45378", "45381", "45382", "45384", "45385", "45387", "45401", "45402", "45403", "45404", "45405", "45406", "45409", "45410", "45412", "45413", "45414", "45415", "45416", "45417", "45419", "45420", "45422", "45423", "45424", "45426", "45428", "45429", "45430", "45431", "45432", "45433", "45434", "45435", "45437", "45439", "45440", "45441", "45448", "45449", "45458", "45459", "45469", "45470", "45475", "45479", "45481", "45482", "45490", "45601", "45612", "45613", "45616", "45617", "45618", "45624", "45628", "45629", "45630", "45633", "45636", "45642", "45644", "45646", "45647", "45648", "45650", "45652", "45653", "45657", "45660", "45661", "45662", "45663", "45671", "45673", "45677", "45679", "45681", "45682", "45683", "45684", "45687", "45690", "45693", "45694", "45697", "45699", "45999"]
# API key for Security Trails
querystring = {"apikey":"7GpQJYMryiL24bulfUWmqiQgeRrExUKx"}


# Querying the API
for postcode in postcodes:    
    payload = "{\"query\":\"whois_country='united states' and  whois_postalCode='" + postcode + "'\"}"
    response = requests.request("POST", url, data=payload, params=querystring)
    json_data = json.loads(response.text)
    
#   Obtaining meta data to identify number of pages and records per postal code
    if 'meta' in json_data:
        max_page = json_data["meta"]["max_page"]
        record_count = json_data['record_count']
        meta_data_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) +"," + xstr(record_count) + "," + xstr(max_page) +"\n") 
    
#   Inner loop --> Going through first page
    if 'records' in json_data:
        for i in range(0, len(json_data['records'])):
            companyName = str(json_data['records'][i]['computed']['company_name']).replace(",", "").replace("/t", "")
            hostName = json_data['records'][i]['hostname']
            expiresDate = json_data['records'][i]['whois']['expiresDate']
            if expiresDate is not None:
                expiresDate = expiresDate * 0.001
                expiresDate = xstr(datetime.utcfromtimestamp(expiresDate).strftime('%Y-%m-%d'))
            else:
                expiresDate = "NA"
                
            createdDate = json_data['records'][i]['whois']['createdDate']
            if createdDate is not None:
                createdDate = createdDate * 0.001
                createdDate = xstr(datetime.utcfromtimestamp(createdDate).strftime('%Y-%m-%d'))
            else:
                createdDate = "NA"
            records_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) + "," + xstr(hostName) + "," + createdDate + "," + expiresDate +"," + xstr(companyName) + "\n" )
#    If pages > 1 --> iterate through remaining pages and records
        if max_page > 1:    
            for pageno in range(2, max_page+1):
                querystring = {"apikey":"7GpQJYMryiL24bulfUWmqiQgeRrExUKx", "page":str(pageno)}
                payload = "{\"query\":\"whois_country='united states' and  whois_postalCode='" + postcode + "'\"}"
                response = requests.request("POST", url, data=payload, params=querystring)
                
                for i in range(0, len(json_data['records'])):
                    companyName = json_data['records'][i]['computed']['company_name']
                    hostName = json_data['records'][i]['hostname']
                    expiresDate = json_data['records'][i]['whois']['expiresDate']
                    if expiresDate is not None:
                        expiresDate = expiresDate * 0.001
                        expiresDate = xstr(datetime.utcfromtimestamp(expiresDate).strftime('%Y-%m-%d'))
                    else:
                        expiresDate = "NA"
                        
                    createdDate = json_data['records'][i]['whois']['createdDate']
                    if createdDate is not None:
                        createdDate = createdDate * 0.001
                        createdDate = xstr(datetime.utcfromtimestamp(createdDate).strftime('%Y-%m-%d'))
                    else:
                        createdDate = "NA"
                    records_file.write(xstr(datetime.today().strftime('%Y-%m-%d')) + "," + xstr(postcode) + "," + xstr(hostName) + "," + createdDate + "," + expiresDate +"," + xstr(companyName) + "\n" )
                
meta_data_file.close()
records_file.close()
```




# Domain Histories
Based on Section (1), we know have the domain names for approximately 131,723 websites that are registered under one of the zip codes corresponding to the counties we explored. In this section, we will explore how each of these domains have changed over time using information from the [Wayback Machine API](https://web.archive.org/). Specifically, we can obtain the number of unique archives for a given webpage using the [waybackpack](https://github.com/jsvine/waybackpack) python-based package for the Wayback Machine API.

## Number of Changes and Archived URLs
As a first step to understanding the domain histories, we capitalized on the [waybackpack](https://github.com/jsvine/waybackpack) to obtain the number of changes for given hostname and the archieved URLs corresponding to unique pages. *For now, we are hypothesizing that the number of changes correspond to growth/economic activity.* **Obviously, we can parse the changes to obtain changes per year or month (we will do this in a future time).** The results from this analysis step are stored in: [wayback_changes.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/wayback_changes.csv).
```{python num_changes, eval=F}
# In this next block of code, we do the following:
# (A) Open the records.csv file from the previous chunk
# (B) Remove links for companies that has more than one link to reduce computational time (with exception of None, NA since it is used for multiple companies)
# (C) Save the resulting data cleaning in: records_unique.csv
with open('../Data/records.csv','r') as in_file, open('../Data/records_unique.csv','w') as out_file:
    seen = set() # set for fast O(1) amortized lookup
    
    for line in in_file:
        cur_company = line.split(",")[5].strip()
        if cur_company in seen: continue # skip duplicate
        if cur_company.lower() != "none" and cur_company is not "NA" and len(cur_company) > 2:
            seen.add(cur_company)
        
        out_file.write(line)

out_file.close()
in_file.close()

# In the next code block, we will capitalize on the subprocess package to run comman line processes so we can capitalize on some of the functionality of the wayback python interface (from the aforementioned GitHub link)


from subprocess import check_output

with open('../Data/records_unique.csv','r') as in_file, open('../Data/wayback_changes.csv','w') as out_file:
    out_file.write("URL, created date, expires date, num changes, wayback URLs\n")
    for line in in_file:
         # Obtaining hostname and dates from records_unique.csv
         url = line.split(",")[2].strip()
         created_date = line.split(",")[3].strip()
         expires_date = line.split(",")[4].strip()
         
         # Ignoring the header
         if  line.split(",")[2].strip() == "hostName":
             continue
         # Running command line using the check_output
         wayback_unique_list = check_output("waybackpack " + url + " --list --uniques-only", shell=True)
         wayback_list = wayback_unique_list.decode('ASCII').strip()
         
         out_file.write(url + "," + created_date + "," + expires_date + ",")
         
         # If we get a URL for the archieve, then we store it and #changes
         if len(wayback_list) != 0:
            wayback_list_as_list = wayback_list.split("\n")
            no_changes = str(len(wayback_list_as_list))
            out_file.write(no_changes + ",")
            # For loop to seperare multiple URLS with semi column
            for wayback_data in wayback_list_as_list:
                 out_file.write(wayback_data.strip() + ";")
            out_file.write("\n")
         else: # #changes equal zero
            out_file.write("0\n")
            
out_file.close()
in_file.close()
```


## Expoloratory Data Analysis on Number of Changes
```{r eda_changes, cache=TRUE}
# Reading the file from the previous chunk
eda_numchanges <- read.csv("../Data/records_unique.csv", stringsAsFactors = F)

eda_numchanges[,2] <- as.character(eda_numchanges[,2]) %>% 
  str_pad(pad="0", side="left", width=5)

# Summarizing Information by Year
eda_numchanges$formatted_CreateDate <- as.Date(eda_numchanges[,4]) %>% format("%Y")
eda_numchanges$formatted_ExpiresDate <- as.Date(eda_numchanges[,5]) %>% format("%Y")


eda_numchanges <- na.omit(eda_numchanges)
eda_numchanges[,c(2,7)] %>% 
  group_by(post_code, formatted_CreateDate) %>% 
  summarise(frequency =n()) %>% datatable()

```


# Categorizing the (Functioning) Websites Using the Web Shrinker API
As a next step, we now attempt to categorize the websites. To categorize the websites, we decided to utilize an online commerical platform for the categorization. We decided to use the [Web Shrinker API](https://docs.webshrinker.com/v3/website-category-api.html#website-category-api-introduction) since: 

(a) The Web Shrinker Category API gets and downloads the content of a given *URL* prior to categorizing it to one of [42 categories](https://docs.webshrinker.com/v3/web-shrinker-categories.html#categories).
(b) Provided more accurate results when compared to the [Website Categorization API from WhoisEXMLAPI](https://website-categorization-api.whoisxmlapi.com/api/) in our initial experiments. 
(c) The API is relatively cheap; $20 allows for 30,000 URL/domain name queries

From the Web Shrinker API, we were able to categorize over 1,100 domains (see [webshrinker.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/webshrinker.csv) for details). The remaining URLs, which were not categorized, were primarily due to the URL expiring, which means that the page cannot be opened and downloaded for further analysis. Note that we have tried using the web archive link for categorization in the Web Shrinker API (i.e. by using the url from the [wayback_changes.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/wayback_changes.csv)) as input to the Python code below, however the results were not satisfactory. We believe that this is due to the structure of the Wayback Machine HTML pages. To examine the errors obtained from the Web Shrinker API, please see [webshrinker_error.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/webshrinker_error.csv).
```{python webshrinker, eval=F}
import requests 
from bs4 import BeautifulSoup 
import unirest


with open('../Data/wayback_changes.csv','r') as in_file, open('../Data/webshrinker.csv','w') as out_file, open('../Data/webshrinker_error.csv','w') as out_file1:
##        out_file.write("scrape_date, post_code, url, created_date, expired_date, company_name,num_change, wayback_URLs, Category1.Confidence, Category1.Score, Category1.ID, Category1.ParentID, Category1.Label, Category2.Confidence, Category2.Score, Category2.ID, Category2.ParentID, Category2.Label, Category3.Confidence, Category3.Score, Category3.ID, Category3.ParentID, Category3.Label, Category4.Confidence, Category4.Score, Category4.ID, Category4.ParentID, Category4.Label \n")
##        out_file1.write("url, Error\n")
    for line in in_file:
        
        if line.startswith("scrape_date"):
            continue
        
##            num_changes = int(line.split(",")[3])
##            if num_changes == 0:
##                contine
        
        url = "http://www." + line.split(",")[2]
        print(url)
       
        #open with GET method
        try:
          
            resp=requests.get(url, timeout = 30)
            
            
            #http_respone 200 means OK status 
            if resp.status_code==200:


                try:
                    from urllib import urlencode
                except ImportError:
                    from urllib.parse import urlencode

                from base64 import urlsafe_b64encode
                import hashlib
                import json

                def webshrinker_categories_v3(access_key, secret_key, url=b"", params={}):
                    params['key'] = access_key

                    request = "categories/v3/{}?{}".format(urlsafe_b64encode(url).decode('utf-8'), urlencode(params, True))
                    request_to_sign = "{}:{}".format(secret_key, request).encode('utf-8')
                    signed_request = hashlib.md5(request_to_sign).hexdigest()

                    return "https://api.webshrinker.com/{}&hash={}".format(request, signed_request)

                access_key = "REf91OgMabbUZTmONupg"
                secret_key = "ZOHNa2dSBvDwpg38QqC0"

                api_url = webshrinker_categories_v3(access_key, secret_key, url)
                response = requests.get(api_url)

                status_code = response.status_code
                data = response.json()

                print(status_code)
                if status_code == 200:
                    # Do something with the JSON response
                   # print(json.dumps(data, indent=4, sort_keys=True))
                    #categories = ""
                    out_file.write(line.strip() + ",")
                    for category in data['data'][0]['categories']:
                        out_file.write(str(category['confident']) + "," + str(category['score']) +"," + str(category['id']) +"," + str(category['parent']) + "," + category['label']+ ",") 
                    out_file.write("\n")
                elif status_code == 202:
                    # The website is being visited and the categories will be updated shortly
##                        print(json.dumps(data, indent=4, sort_keys=True))
                    out_file1.write(url + ",uncategorized\n") 
                elif status_code == 400:
                    # Bad or malformed HTTP request
                    out_file1.write(url + ",Bad or malformed HTTP request\n")
                elif status_code == 401:
                    # Unauthorized
                    out_file1.write(url + ",Unauthorized - check your access and secret key permissions\n")
                elif status_code == 402:
                    # Request limit reached
                    print("Account request limit reached")
                    print(json.dumps(data, indent=4, sort_keys=True))
                else:
                    # General error occurred
                    print("A general error occurred, try the request again")
            else:
                out_file1.write(url + ", " + str(resp.status_code) + "\n")
                
        except requests.exceptions.RequestException as e:  # This is the correct syntax
            out_file1.write(url + "," + str(e).replace(",", ";") + "\n")
            
out_file.close()
out_file1.close()
in_file.close()
```

# Transforming Webshrinker and Wayback Machine Data to a Longitudinal Panel
Since our end goal is to examine how the number of unique changes of websites in a given category correlate with economic activity, we used Python to transform the outputs from the previous sections into a panel-like CSV. The code chunk explains our thought process and the output can be downloaded from[website_change_panel_v01.csv](https://github.com/fmegahed/entrepreneurship-in-southwest-ohio/blob/master/Data/website_change_panel_v01.csv).
```{python web_changes_panel, eval=F}
import requests 
from bs4 import BeautifulSoup 
import unirest

start_year = 1996
end_year = 2021

def title(): 
    # the target we want to open   

    urls_visted = set()
    
    with open('../Data/webshrinker.csv','r') as in_file, open('../Data/website_change_panel_v01.csv','w') as out_file:
        out_file.write("scrape_date, ZIP, url, created_date, expired_date, company_name, TAG_category, category1_confidence, category1_score, category1_ID, category1_parentID, category1_label, category2_confidence, category2_score, category2_ID, category2_parentID, category2_label, wayback_URLs, num_changes, " + ', '.join(["changes_in_"+str(i) for i in range(start_year, end_year)]) + "\n")
        
        for line in in_file:
            
            if line.startswith("scrape_date"):
                continue

            line_col = line.split(",")
            urls_visted.add(line_col[2])
            
            for i in range(6):
                out_file.write(line_col[i] + ",")

            TAG_category = "Category exists"
            if line_col[12] == "Uncategorized" or line_col[17] == "Under Construction":
                TAG_category = "Uncategorized or Under Construction"
            out_file.write(TAG_category + ",")

            for i in range(8, 18):
                out_file.write(line_col[i] + ",")


            wayback_urls = line_col[7]
            num_changes = int(line_col[6])
            out_file.write(wayback_urls + "," + str(num_changes) +",")

            if num_changes > 0:
                wayback_urls_list = wayback_urls.split(";")
                

                count = [0] * (end_year - start_year)
                for i in range(num_changes):
                    url = wayback_urls_list[i]                    
                    year = int(url.split("/")[4][0:4])
                    count[year - start_year] += 1

                out_file.write(', '.join(str(x) for x in count))
                
            out_file.write("\n")

    out_file.close()
    in_file.close()



    with open('../Data/wayback_changes.csv','r') as in_file, open('../Data/website_change_panel_v01.csv','a') as out_file:
        for line in in_file:
            
            if line.startswith("scrape_date"):
                continue

            line_col = line.split(",")
            
            if line_col[2] in urls_visted:
                continue

            for i in range(6):
                out_file.write(line_col[i] + ",")

            out_file.write("Webshrinker Error,")

            for i in range(8, 18):
                out_file.write(",")

            wayback_urls = line_col[7].strip()
            num_changes = int(line_col[6])
            out_file.write(wayback_urls + "," + str(num_changes) +",")

            if num_changes > 0:
                wayback_urls_list = wayback_urls.split(";")
                

                count = [0] * (end_year - start_year)
                for i in range(num_changes):
                    url = wayback_urls_list[i]                    
                    year = int(url.split("/")[4][0:4])
                    count[year - start_year] += 1

                out_file.write(', '.join(str(x) for x in count))
                
            out_file.write("\n")

    out_file.close()
    in_file.close()
                

title()
```